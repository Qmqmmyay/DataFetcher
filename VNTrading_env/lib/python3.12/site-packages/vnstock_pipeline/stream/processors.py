"""
Data Processors
==============

Classes for processing streaming market data.
"""

import csv
import json
import logging
import os
import time
import traceback
import datetime
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Tuple, Set, Optional

class DataProcessor(ABC):
    """
    Base abstract class for processing received market data.
    
    All data processors must inherit from this class and implement the
    process method to handle incoming data.
    """
    
    def __init__(self):
        """Initialize the data processor."""
        self.logger = logging.getLogger(self.__class__.__name__)
    
    @abstractmethod
    async def process(self, data: Dict[str, Any]) -> None:
        """
        Process the received data.
        
        Args:
            data (Dict[str, Any]): The parsed market data
        """
        pass


class ConsoleProcessor(DataProcessor):
    """Process data by printing to console for debugging and monitoring."""
    
    def __init__(self, pretty_print: bool = True):
        """
        Initialize the console processor.
        
        Args:
            pretty_print (bool): Whether to pretty-print the JSON output
        """
        super().__init__()
        self.pretty_print = pretty_print
    
    async def process(self, data: Dict[str, Any]) -> None:
        """
        Print the received data to console.
        
        Args:
            data (Dict[str, Any]): The parsed market data
        """
        try:
            if self.pretty_print:
                print(f"Received: {json.dumps(data, indent=2)}")
            else:
                print(f"Received: {data}")
        except Exception as e:
            self.logger.error(f"Error in ConsoleProcessor: {e}")
            self.logger.error(traceback.format_exc())


class CSVProcessor(DataProcessor):
    """Process data by saving to CSV files, organized by data type."""
    
    def __init__(self, filename_template: str = "market_data_{data_type}_%Y-%m-%d.csv"):
        """
        Initialize the CSV processor.
        
        Args:
            filename_template (str): Template for CSV filenames with formatting placeholders
                                     for data_type and date
        """
        super().__init__()
        self.filename_template = filename_template
        self.files = {}  # Cache for file handlers
        self.fields = {}  # Cache for field names by filename
        
    def _get_filename(self, data: Dict[str, Any]) -> str:
        """
        Generate filename based on data type and date.
        
        Args:
            data (Dict[str, Any]): The parsed market data
            
        Returns:
            str: The generated filename
        """
        data_type = data.get("data_type", "unknown")
        filename = self.filename_template.replace("{data_type}", data_type)
        filename = datetime.datetime.now().strftime(filename)
        return filename

    def _create_new_file(self, filename: str, fields: List[str]):
        """Create a new CSV file with the given fields."""
        try:
            import csv
            os.makedirs(os.path.dirname(filename) or '.', exist_ok=True)
            file = open(filename, 'w', newline='')
            writer = csv.DictWriter(file, fieldnames=fields)
            writer.writeheader()
            self.files[filename] = (file, writer)
            self.fields[filename] = set(fields)
            self.logger.info(f"Created new CSV file: {filename}")
            return file, writer
        except Exception as e:
            self.logger.error(f"Error creating CSV file {filename}: {e}")
            self.logger.error(traceback.format_exc())
            raise
    
    async def process(self, data: Dict[str, Any]) -> None:
        """
        Save the received data to CSV file.
        
        Args:
            data (Dict[str, Any]): The parsed market data
        """
        import csv
        
        try:
            filename = self._get_filename(data)
            
            # Get all fields from data
            current_fields = set(data.keys())
            
            # Check if we have a file handler already
            if filename in self.files:
                file, writer = self.files[filename]
                existing_fields = self.fields[filename]
                
                # Check if we have new fields not in the existing schema
                new_fields = current_fields - existing_fields
                if new_fields:
                    self.logger.info(f"Found {len(new_fields)} new fields in data. Creating new CSV file.")
                    # Close the existing file
                    file.close()
                    
                    # Create a new filename with timestamp to avoid overwriting
                    timestamp = int(time.time())
                    new_filename = f"{os.path.splitext(filename)[0]}_{timestamp}.csv"
                    
                    # Get all fields (old + new)
                    all_fields = list(existing_fields.union(current_fields))
                    
                    # Create new file with all fields
                    file, writer = self._create_new_file(new_filename, all_fields)
                    
                    # Update references
                    self.files[filename] = (file, writer)
                    self.fields[filename] = set(all_fields)
            else:
                # No existing file, create a new one
                file, writer = self._create_new_file(filename, list(data.keys()))
            
            # Write data, filling in missing values with None
            row_data = {field: data.get(field, None) for field in self.fields[filename]}
            writer.writerow(row_data)
            file.flush()  # Ensure data is written to disk
            
        except Exception as e:
            self.logger.error(f"Error writing to CSV: {e}")
            self.logger.error(traceback.format_exc())
    
    def close(self):
        """Close all open file handlers."""
        for filename, (file, _) in self.files.items():
            try:
                file.close()
                self.logger.debug(f"Closed file: {filename}")
            except Exception as e:
                self.logger.error(f"Error closing file {filename}: {e}")


class DuckDBProcessor(DataProcessor):
    """Process data by saving to DuckDB with schema evolution support."""
    
    def __init__(self, db_path: str, table_prefix: str = "market_data"):
        """
        Initialize the DuckDB processor.
        
        Args:
            db_path (str): Path to the DuckDB database file
            table_prefix (str): Prefix for table names
        """
        super().__init__()
        try:
            import duckdb
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(db_path) or '.', exist_ok=True)
            self.con = duckdb.connect(db_path)
            self.table_prefix = table_prefix
            self.tables = {}  # Track tables and their schemas
            self.logger.info(f"DuckDB initialized: {db_path}")
        except ImportError:
            self.logger.error("DuckDB module not installed. Install with 'pip install duckdb'")
            raise
        except Exception as e:
            self.logger.error(f"DuckDB initialization error: {e}")
            self.logger.error(traceback.format_exc())
            raise
    
    def _get_table_name(self, data: Dict[str, Any]) -> str:
        """
        Generate table name based on data type.
        
        Args:
            data (Dict[str, Any]): The parsed market data
            
        Returns:
            str: The generated table name
        """
        data_type = data.get("data_type", "unknown")
        return f"{self.table_prefix}_{data_type}"
    
    def _ensure_table_exists(self, table_name: str, data: Dict[str, Any]) -> None:
        """
        Ensure table exists with the correct schema.
        
        This method creates the table if it doesn't exist or adds new columns
        as needed to support schema evolution.
        
        Args:
            table_name (str): The table name
            data (Dict[str, Any]): The data to be inserted
        """
        try:
            # Check if table exists in our cache
            if table_name in self.tables:
                # Table exists, check for new columns
                current_columns = self.tables[table_name]
                for key, value in data.items():
                    if key not in current_columns:
                        # Add new column
                        data_type = self._infer_type(value)
                        query = f'ALTER TABLE "{table_name}" ADD COLUMN "{key}" {data_type}'
                        try:
                            self.con.execute(query)
                            self.tables[table_name].add(key)
                            self.logger.info(f"Added column {key} to {table_name}")
                        except Exception as e:
                            self.logger.error(f"Failed to add column {key}: {e}")
                            self.logger.error(traceback.format_exc())
            else:
                # Check if table exists in database
                try:
                    self.con.execute(f'SELECT * FROM "{table_name}" LIMIT 0')
                    # Table exists, get schema
                    result = self.con.execute(f'PRAGMA table_info("{table_name}")')
                    columns = {row[1] for row in result.fetchall()}
                    self.tables[table_name] = columns
                    
                    # Now check for new columns
                    for key, value in data.items():
                        if key not in columns:
                            # Add new column
                            data_type = self._infer_type(value)
                            query = f'ALTER TABLE "{table_name}" ADD COLUMN "{key}" {data_type}'
                            try:
                                self.con.execute(query)
                                self.tables[table_name].add(key)
                                self.logger.info(f"Added column {key} to {table_name}")
                            except Exception as e:
                                self.logger.error(f"Failed to add column {key}: {e}")
                                self.logger.error(traceback.format_exc())
                    
                except Exception:
                    # Table doesn't exist, create it
                    columns = []
                    table_columns = set()
                    for key, value in data.items():
                        data_type = self._infer_type(value)
                        columns.append(f'"{key}" {data_type}')
                        table_columns.add(key)
                    
                    query = f'CREATE TABLE "{table_name}" ({", ".join(columns)})'
                    self.con.execute(query)
                    self.tables[table_name] = table_columns
                    self.logger.info(f"Created table {table_name}")
                    
        except Exception as e:
            self.logger.error(f"Error ensuring table exists: {e}")
            self.logger.error(traceback.format_exc())
            raise
    
    def _infer_type(self, value: Any) -> str:
        """
        Infer SQL data type from Python value.
        
        Args:
            value (Any): The Python value
            
        Returns:
            str: The SQL data type
        """
        if value is None:
            return "VARCHAR"  # Use VARCHAR for NULL values
        elif isinstance(value, bool):
            return "BOOLEAN"
        elif isinstance(value, int):
            # Use BIGINT for all integers to avoid overflow
            return "BIGINT"
        elif isinstance(value, float):
            return "DOUBLE"
        elif isinstance(value, str):
            return "VARCHAR"
        elif isinstance(value, (list, dict)):
            return "JSON"
        elif isinstance(value, datetime.datetime):
            return "TIMESTAMP"
        elif isinstance(value, datetime.date):
            return "DATE"
        else:
            return "VARCHAR"  # Default
    
    def _sanitize_value(self, value: Any) -> Any:
        """
        Sanitize values to ensure they're compatible with DuckDB.
        
        Args:
            value (Any): The value to sanitize
            
        Returns:
            Any: The sanitized value
        """
        if isinstance(value, int) and (value > 2147483647 or value < -2147483648):
            # Convert large integers to strings to avoid INT32 overflow
            return str(value)
        return value
    
    async def process(self, data: Dict[str, Any]) -> None:
        """
        Save the received data to DuckDB.
        
        Args:
            data (Dict[str, Any]): The parsed market data
        """
        try:
            # Get table name
            table_name = self._get_table_name(data)
            
            # Ensure table exists with correct schema
            self._ensure_table_exists(table_name, data)
            
            # Filter data to only include known columns
            if table_name in self.tables:
                filtered_data = {k: v for k, v in data.items() if k in self.tables[table_name]}
            else:
                filtered_data = data
            
            if not filtered_data:
                self.logger.warning(f"No valid columns to insert into {table_name}")
                return
            
            # Sanitize values to avoid conversion errors
            sanitized_data = {k: self._sanitize_value(v) for k, v in filtered_data.items()}
            
            # Insert data
            columns = ", ".join([f'"{k}"' for k in sanitized_data.keys()])
            placeholders = ", ".join(["?" for _ in sanitized_data.keys()])
            values = list(sanitized_data.values())
            
            insert_query = f'INSERT INTO "{table_name}" ({columns}) VALUES ({placeholders})'
            self.con.execute(insert_query, values)
            self.logger.debug(f"Inserted data into {table_name}")
            
        except Exception as e:
            self.logger.error(f"Error inserting data into DuckDB: {e}")
            self.logger.error(traceback.format_exc())
    
    def close(self):
        """Close the database connection."""
        try:
            self.con.close()
            self.logger.info("DuckDB connection closed")
        except Exception as e:
            self.logger.error(f"Error closing DuckDB connection: {e}")

class FirebaseProcessor(DataProcessor):
    """Process data by saving to Firebase Firestore."""
    
    def __init__(self, collection_prefix: str = "market_data", service_account_path: str = None):
        """
        Initialize the Firebase processor.
        
        Args:
            collection_prefix (str): Prefix for collection names
            service_account_path (str): Path to the Firebase service account key file
        """
        super().__init__()
        try:
            import firebase_admin
            from firebase_admin import credentials, firestore
            
            # Initialize Firebase if not already initialized
            if not firebase_admin._apps:
                if service_account_path:
                    cred = credentials.Certificate(service_account_path)
                else:
                    cred = credentials.ApplicationDefault()
                firebase_admin.initialize_app(cred)
            
            self.db = firestore.client()
            self.collection_prefix = collection_prefix
            self.logger.info(f"Firebase Firestore initialized with prefix: {collection_prefix}")
        except ImportError:
            self.logger.error("Firebase modules not installed. Install with 'pip install firebase-admin'")
            raise
        except Exception as e:
            self.logger.error(f"Firebase initialization error: {e}")
            self.logger.error(traceback.format_exc())
            raise
    
    def _get_collection_name(self, data: Dict[str, Any]) -> str:
        """
        Generate collection name based on data type.
        
        Args:
            data (Dict[str, Any]): The parsed market data
            
        Returns:
            str: The generated collection name
        """
        data_type = data.get("data_type", "unknown")
        return f"{self.collection_prefix}_{data_type}"
    
    def _get_document_id(self, data: Dict[str, Any]) -> str:
        """
        Generate document ID based on data.
        
        Args:
            data (Dict[str, Any]): The parsed market data
            
        Returns:
            str: The generated document ID
        """
        # Use timestamp as base for document ID
        timestamp = data.get("timestamp", time.time())
        
        # For price data, include the ticker
        if data.get("data_type") == "price" and "ticker" in data:
            return f"{data['ticker']}_{timestamp}"
        # For index data, include the index ID
        elif data.get("data_type") == "index" and "index_id" in data:
            return f"{data['index_id']}_{timestamp}"
        else:
            # For other data types, just use timestamp
            return f"{timestamp}"
    
    async def process(self, data: Dict[str, Any]) -> None:
        """
        Save the received data to Firestore.
        
        Args:
            data (Dict[str, Any]): The parsed market data
        """
        try:
            # Get collection name
            collection_name = self._get_collection_name(data)
            
            # Get document ID
            doc_id = self._get_document_id(data)
            
            # Add to Firestore
            self.db.collection(collection_name).document(doc_id).set(data)
            self.logger.debug(f"Data saved to Firestore: {collection_name}/{doc_id}")
        except Exception as e:
            self.logger.error(f"Error saving to Firestore: {e}")
            self.logger.error(traceback.format_exc())